{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ff3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\reggy\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from seaborn) (1.9.1)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from seaborn) (1.21.5)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: imdbpy in c:\\users\\reggy\\anaconda3\\lib\\site-packages (2022.7.9)\n",
      "Requirement already satisfied: cinemagoer in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from imdbpy) (2022.2.11)\n",
      "Requirement already satisfied: SQLAlchemy in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from cinemagoer->imdbpy) (1.4.39)\n",
      "Requirement already satisfied: lxml in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from cinemagoer->imdbpy) (4.9.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from SQLAlchemy->cinemagoer->imdbpy) (1.1.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\reggy\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\reggy\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn \n",
    "!pip install imdbpy\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import time\n",
    "from numpy import array\n",
    "from os.path import exists\n",
    "import imdb\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4c014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting out arguments\n",
    "userId = 3223\n",
    "personalised = True\n",
    "entry = [{'userId':userId +1, 'movieId':4993, 'rating': 4.5, 'timestamp': 1147880044},\n",
    "             {'userId':userId +1, 'movieId':7153, 'rating':5.0, 'timestamp': 1147880044},\n",
    "             {'userId':userId +1, 'movieId':98809, 'rating':4.0, 'timestamp':1147880044},\n",
    "             {'userId':userId +1, 'movieId':106489, 'rating':5.0, 'timestamp':1147880044}]\n",
    "entry = []\n",
    "size = 1000000\n",
    "test = True\n",
    "personalised = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b073558",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c623c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMean(ratings):\n",
    "    # Calculating mean of ratings \n",
    "    meanRating = ratings.groupby('movieId')['rating'].mean()\n",
    "    countRating = ratings['movieId'].value_counts().sort_index()\n",
    "\n",
    "    # Combine the series\n",
    "    df = pd.concat([meanRating, countRating], axis=1).reset_index()\n",
    "    df = df.rename(columns={\"index\": \"movieId\", \"movieId\": \"count\"})\n",
    "    \n",
    "    # drop movies rated less than 10 times\n",
    "    df.drop(df[df['count'] < 10].index, inplace = True)\n",
    "    return df\n",
    "\n",
    "def makeRatings(ratings,entry,size):\n",
    "    # reduce dataset to 5m \n",
    "    ratings.drop(ratings.index[size:25000095], inplace=True)\n",
    "    if entry != []:\n",
    "        \n",
    "        # new entry for testing\n",
    "        ratings = ratings.append(entry, ignore_index=True)\n",
    "\n",
    "    # make all userIds start at 0\n",
    "    ratings['userId'] = ratings['userId'].apply(lambda x: x-1)\n",
    "    #print('ratings shape = ',ratings.shape)\n",
    "    \n",
    "    df = calcMean(ratings)\n",
    "    #print('mean ratings shape = ',df.shape)\n",
    "    \n",
    "    return ratings, df\n",
    "\n",
    "\n",
    "ratings, df = makeRatings(pd.read_csv('ml-25m/ratings.csv'),entry,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22ab5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMovies(movies):\n",
    "    # read and make genres into a list\n",
    "    movies['genres'] = movies['genres'].apply(lambda x : x.split('|'))\n",
    "    \n",
    "    #print('movies shape = ',movies.shape)\n",
    "    return movies\n",
    "\n",
    "def getMapping(movies):\n",
    "    oldmovies = movies['movieId']\n",
    "    movies = movies.rename(columns={'movieId': 'oldMovieId'})\n",
    "    \n",
    "    # remove movies rated less than 10 times and reindex \n",
    "    movies.drop(movies[~movies.oldMovieId.isin(df.movieId)].index, inplace=True)\n",
    "    movies['movieId'] = np.arange(df.shape[0])\n",
    "    \n",
    "    # reorders columns\n",
    "    cols = movies.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    movies = movies[cols]\n",
    "    mapping = dict(zip(movies['oldMovieId'], movies['movieId']))\n",
    "    #movies.drop('oldMovieId', inplace=True, axis=1)\n",
    "    \n",
    "    #print('movies shape = ',movies.shape)\n",
    "    return movies, mapping\n",
    "    \n",
    "movies = makeMovies(pd.read_csv('ml-25m/movies.csv'))\n",
    "movies, mapping = getMapping(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1cff285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex all the tables store as csv for fast completion\n",
    "if not exists('s' + str(size)+'ratings.csv'):\n",
    "    ratings.drop(ratings[~ratings.movieId.isin(df.movieId)].index, inplace = True)\n",
    "    ratings.replace({'movieId': mapping}, inplace = True)\n",
    "    ratings.to_csv('s' + str(size)+'ratings.csv')\n",
    "else:\n",
    "    ratings = pd.read_csv('s' + str(size)+'ratings.csv', index_col=[0])\n",
    "\n",
    "links = pd.read_csv('ml-25m/genome-tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74e5c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>oldMovieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7440.000000</td>\n",
       "      <td>7440.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3719.500000</td>\n",
       "      <td>34125.117742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2147.887334</td>\n",
       "      <td>48601.672617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1859.750000</td>\n",
       "      <td>2508.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3719.500000</td>\n",
       "      <td>5668.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5579.250000</td>\n",
       "      <td>57969.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7439.000000</td>\n",
       "      <td>204698.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           movieId     oldMovieId\n",
       "count  7440.000000    7440.000000\n",
       "mean   3719.500000   34125.117742\n",
       "std    2147.887334   48601.672617\n",
       "min       0.000000       1.000000\n",
       "25%    1859.750000    2508.500000\n",
       "50%    3719.500000    5668.500000\n",
       "75%    5579.250000   57969.500000\n",
       "max    7439.000000  204698.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1896df41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "2176    3221\n",
       "1747    2862\n",
       "2981    2376\n",
       "547     2348\n",
       "3149    2309\n",
       "        ... \n",
       "279      300\n",
       "4060     299\n",
       "4903     299\n",
       "5119     298\n",
       "2690     298\n",
       "Name: rating, Length: 800, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.groupby(['userId']).count().rating.sort_values(ascending = False).head(800)\n",
    "\n",
    "\n",
    "# 2176 - 3221\n",
    "# 4740 - 1224\n",
    "# 3402 - 919\n",
    "# 5440 - 689\n",
    "# 356 - 578\n",
    "# 1721 - 419\n",
    "# 2964 - 248\n",
    "# 4895 - 158 \n",
    "# 1140 - 130\n",
    "# 20 - 103\n",
    "# 3691 - 97\n",
    "# 4157 - 84\n",
    "# 1451 - 70\n",
    "# 2048 - 65\n",
    "# 3737 - 59\n",
    "# 3223 - 52\n",
    "# 2792 - 25\n",
    "\n",
    "# 5235 62\n",
    "# 5173 65\n",
    "# 1286 67\n",
    "# 977 73\n",
    "# 5774 76\n",
    "# 2608 84\n",
    "\n",
    "# 5595 117\n",
    "# 5077 106\n",
    "# 5757 145\n",
    "# 1971 162\n",
    "# 5791 173\n",
    "# 6327 212\n",
    "# 4480 198\n",
    "# 279 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5d5f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6746"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.userId.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb1fdd",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed19af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/nikitaa30/Recommender-Systems/blob/master/matrix_factorisation_svd.py\n",
    "\n",
    "# this is why all the movies are reindexed\n",
    "ratings_mat = ratings.pivot(\n",
    "    index='userId',\n",
    "    columns='movieId',\n",
    "    values='rating'\n",
    ").fillna(0)\n",
    "\n",
    "R = ratings_mat.values\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "U, sigma, Vt = svds(R_demeaned, k = 50)\n",
    "\n",
    "sigma = np.diag(sigma)\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, columns = ratings_mat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf2bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to use the prediction score\n",
    "def recommend_movies(preds_df, userID, movies_df, original_ratings_df, num_recommendations=5):\n",
    "    user_row_number = userID - 1 # UserID starts at 0\n",
    "    sorted_user_predictions = preds_df.iloc[user_row_number].sort_values(ascending=False) \n",
    "    user_data = original_ratings_df[original_ratings_df.userId == (userID)]\n",
    "    \n",
    "    user_full = (user_data.merge(movies_df, how = 'left', left_on = 'movieId', right_on = 'movieId').sort_values(['rating'], ascending=False))\n",
    "    if test == True:\n",
    "        # halves the amount of ratings for testing\n",
    "        user_full = user_full[:-int(user_full.shape[0])//2]\n",
    "    recommendations = (movies_df[~movies_df['movieId'].isin(user_full['movieId'])]).merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left', left_on = 'movieId',right_on = 'movieId').rename(columns = {user_row_number: 'Predictions'}).sort_values('Predictions', ascending = False).iloc[:num_recommendations, :-1]\n",
    "    \n",
    "    return user_full, recommendations, sorted_user_predictions\n",
    "\n",
    "already_rated, predictions, sorted_user_predictions = recommend_movies(preds_df, userId, movies[['movieId','title']], ratings, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f3bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_user_predictions = pd.DataFrame(sorted_user_predictions).reset_index()\n",
    "sorted_user_predictions['title'] = [movies[movies.movieId == x].title.values[0] for x in sorted_user_predictions['movieId']]\n",
    "sorted_user_predictions.rename(columns={sorted_user_predictions.columns[1]: \"predictedEvaluation\"}, inplace = True)\n",
    "predictions['predictedEvaluation'] = [sorted_user_predictions[sorted_user_predictions.movieId == x].predictedEvaluation.values[0] for x in predictions['movieId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2980faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the movies used for CBF\n",
    "testmovies = movies[movies.movieId.isin(predictions.movieId) | movies.movieId.isin(already_rated.movieId)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57260933",
   "metadata": {},
   "source": [
    "## Do TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bdb251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\reggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\reggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\reggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\reggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\reggy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# natural language processing initialisation \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "  \n",
    "VERB_CODES = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0968af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gets the plot outline and finds the 3 most relevant words\n",
    "def getDescription(row):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        search = ia.get_movie(row[2]).get('plot outline')\n",
    "        row[2] = search.lower()\n",
    "\n",
    "        temp_sent =[]\n",
    "        words = nltk.word_tokenize(row[2])\n",
    "        tags = nltk.pos_tag(words)\n",
    "        for i, word in enumerate(words):\n",
    "            if tags[i][1] in VERB_CODES: \n",
    "                lemmatized = lemmatizer.lemmatize(word, 'v')\n",
    "            else:\n",
    "                lemmatized = lemmatizer.lemmatize(word)\n",
    "            if lemmatized not in stop_words and lemmatized.isalpha():\n",
    "                temp_sent.append(lemmatized)\n",
    "        finalsent = ' '.join(temp_sent)\n",
    "        finalsent = finalsent.replace(\"n't\", \" not\")\n",
    "        finalsent = finalsent.replace(\"'m\", \" am\")\n",
    "        finalsent = finalsent.replace(\"'s\", \" is\")\n",
    "        finalsent = finalsent.replace(\"'re\", \" are\")\n",
    "        finalsent = finalsent.replace(\"'ll\", \" will\")\n",
    "        finalsent = finalsent.replace(\"'ve\", \" have\")\n",
    "        finalsent = finalsent.replace(\"'d\", \" would\")\n",
    "        row[2] = finalsent.split(' ')[:3]\n",
    "    except:\n",
    "        row[2] = []\n",
    "        \n",
    "    print(time.time()-start)\n",
    "    return row\n",
    "\n",
    "# takes very long to compute when theres lots of predictions so we store as csv\n",
    "if not exists(str(test) +'u' + str(userId) + 's' + str(size)+'testmovies.csv'):\n",
    "    # only get info for rated and predicted movies\n",
    "    movieLinks = pd.read_csv('ml-25m/links.csv')\n",
    "    movieLinks = movieLinks[movieLinks.movieId.isin(testmovies.oldMovieId)]\n",
    "    movieLinks.replace({'movieId': mapping}, inplace = True)\n",
    "    ia = imdb.IMDb()\n",
    "    movieLinks = movieLinks.astype({'tmdbId':str})\n",
    "    movieLinks = movieLinks.apply(getDescription, axis = 1)\n",
    "    movieLinks.rename(columns={\"tmdbId\": \"description\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35c9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turns the tagIds into tags \n",
    "def find(row):\n",
    "    for x in range(len(row[1])):\n",
    "        row[1][x] = links.loc[links['tagId'] == row[1][x]].tag.values[0]\n",
    "    return row\n",
    "\n",
    "# collects all the tags, genres and descriptive key words\n",
    "def combine(row):\n",
    "    tags = taglist.loc[taglist['movieId'] == row[0]].tag.values\n",
    "    desc = movieLinks.loc[movieLinks['movieId']== row[0]].description.values\n",
    "    \n",
    "    # checks to see if theres any movie data\n",
    "    if len(desc) > 0 and len(tags) > 0:\n",
    "        # returns as string for some reason so have to fix\n",
    "        if isinstance(desc[0], str):\n",
    "            desc[0] = desc[0].strip('][').replace('\\'','').split(', ')\n",
    "        newrow = list(dict.fromkeys(tags[0]+row[3]+desc[0]))\n",
    "        \n",
    "    elif len(desc) > 0:\n",
    "        if isinstance(desc[0], str):\n",
    "            desc[0] = desc[0].strip('][').replace('\\'','').split(', ')\n",
    "        newrow = list(dict.fromkeys(row[3] + desc[0]))\n",
    "        \n",
    "    elif len(tags) > 0:\n",
    "        newrow = list(dict.fromkeys(tags[0]+row[3]))\n",
    "        \n",
    "    else:\n",
    "        newrow = list(dict.fromkeys(row[3]))\n",
    "    \n",
    "    # turns back into a string\n",
    "    row[3] = ' '.join(newrow).lower()\n",
    "    return row\n",
    "\n",
    "# gets the 3 most relevant tags\n",
    "def getTop3Tags(tags):\n",
    "    taglist = tags.groupby('movieId', as_index =False).apply(lambda x: x.sort_values(by='relevance',ascending = False)[0:3])\n",
    "    taglist = taglist.groupby('movieId')['tagId'].apply(list).reset_index()\n",
    "    taglist = pd.DataFrame(taglist)\n",
    "    taglist.columns.values[1] = 'tag'\n",
    "    taglist = taglist.apply(find, axis = 1)\n",
    "    return taglist\n",
    "\n",
    "# only get the tags for rated and predicted movies and reindex movieId\n",
    "if not exists(str(test) + 'u' + str(userId) + 's' + str(size)+'testmovies.csv'):\n",
    "    tags = pd.read_csv('ml-25m/genome-scores.csv')\n",
    "    tags = tags[tags.movieId.isin(testmovies.oldMovieId)]\n",
    "    tags.replace({'movieId': mapping}, inplace = True)\n",
    "    taglist = getTop3Tags(tags)\n",
    "    testmovies = testmovies.apply(combine, axis =1)\n",
    "    testmovies.to_csv(str(test) + 'u' + str(userId) + 's' + str(size)+'testmovies.csv')\n",
    "    \n",
    "else:\n",
    "    testmovies = pd.read_csv(str(test) + 'u' + str(userId) + 's' + str(size)+'testmovies.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb84fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do TFIDF\n",
    "tf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(testmovies['genres'])\n",
    "\n",
    "# create the cosine similarity matrix\n",
    "sim_matrix = linear_kernel(tfidf_matrix,tfidf_matrix)\n",
    "\n",
    "# Done for reference to sim_matrix\n",
    "testmovies = testmovies.reset_index()\n",
    "testmovies.drop(testmovies.columns[0], axis =1,inplace = True)\n",
    "\n",
    "simMetric = []\n",
    "simMovie = []\n",
    "i = 0\n",
    "\n",
    "# calculates similarity from predictions and already rated\n",
    "for index, row in predictions.iterrows():\n",
    "    # finds the position of the movie in the sim_matrix\n",
    "    movieIndex = testmovies[testmovies.movieId == row.movieId].movieId.index.values[0]\n",
    "    temp = sim_matrix[movieIndex]\n",
    "    \n",
    "    # makes the diagonal = 0 so doesnt compare with itself\n",
    "    temp[movieIndex] = 0\n",
    "    \n",
    "    simMetric.append(np.max(temp))\n",
    "    basedOn = testmovies.iloc[np.argmax(temp)]\n",
    "    \n",
    "    # if the movie is most similar to aready rated movie we store it \n",
    "    try:\n",
    "        simMovie.append(already_rated[already_rated.movieId == basedOn.movieId].title.values[0])\n",
    "    except:\n",
    "        simMovie.append(np.nan)\n",
    "    i = i + 1 \n",
    "    \n",
    "predictions['similarity'] = simMetric\n",
    "predictions['basedOn'] = simMovie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685df6a6",
   "metadata": {},
   "source": [
    "## Expert System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d6d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts the average rating and count in the prediction table\n",
    "averageRating = calcMean(ratings)\n",
    "predictions['averageRatings'] = [averageRating[averageRating.movieId == x].rating.values[0] for x in predictions['movieId']]\n",
    "predictions['count'] = [averageRating[averageRating.movieId == x]['count'].values[0] for x in predictions['movieId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5952e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the importance by product of the normalised values\n",
    "def importance(row):\n",
    "    return row[3]/predictions['similarity'].max() * row[5]/predictions['averageRatings'].max() * row[6]/predictions['count'].max()\n",
    "predictions['importance'] = predictions.apply(importance, axis=1)\n",
    "predictions['finalEval'] = predictions['importance']*predictions['predictedEvaluation']\n",
    "predictions = predictions.sort_values('finalEval',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ffc5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the actual predictedEvaluation represent a score /5\n",
    "meanPred = predictions['predictedEvaluation'].mean()\n",
    "meanUserRating = ratings[ratings.userId == userId].rating.mean()\n",
    "\n",
    "# helps to normlalise the rows closer to the average user rating\n",
    "def normalise(row):\n",
    "    # mean of our predictions should be the average user rating\n",
    "    ratio = meanUserRating/meanPred\n",
    "    row[2] = row[2] * ratio \n",
    "    \n",
    "    # brings it closer to the mean user rating\n",
    "    if row[2] > meanUserRating:\n",
    "        row[2] = row[2] - row[2]*ratio*0.1\n",
    "        \n",
    "    if row[2] < meanUserRating:\n",
    "        row[2] = row[2] + row[2]*ratio*0.1\n",
    "        \n",
    "    if row[2] > 5:\n",
    "        row[2] = 5\n",
    "        \n",
    "    # round to nearest half \n",
    "    row[2] = round(row[2] * 2) / 2\n",
    "    return row[2]\n",
    "\n",
    "predictions['predictedEvaluation'] = predictions.apply(normalise, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a2cf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(\\'number of correctly predicted movies =\\' ,evalCount*20)\\nprint(\\'accuracy =\\',evalCount*100, \"%\")\\nprint(\\'mean average error of rating predictions = \\', mae)\\nprint(\\'percentage of explainable recommendations =\\',explan *100,\\'%\\')\\nprint(\\'diversity score =\\',diversity*100, \"%\")\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if test == True:\n",
    "    \n",
    "    # gets the test table so can add a new result\n",
    "    if not exists('s' + str(size)+'testTable.csv'):\n",
    "        testTable = pd.DataFrame()\n",
    "    else:\n",
    "        testTable = pd.read_csv('s' + str(size)+'testTable.csv',index_col=0, names =['userId','numPredictions', 'accuracy', 'meanAverageError','explainability','diversity'] )\n",
    "        # duplicates the column names for some reason\n",
    "        testTable = testTable.iloc[1: , :]\n",
    "        \n",
    "    ogRatings = ratings[ratings.userId == (userId)]\n",
    "    evalCount = 0\n",
    "    mae = 0\n",
    "    diversity = 0\n",
    "    numPredictions = ratings.groupby(['userId']).count()[ratings.groupby(['userId']).count().index == userId].rating.values[0]\n",
    "    explan = predictions[\"basedOn\"].count() / len(predictions)\n",
    "    avSim = predictions['similarity'].mean()\n",
    "    \n",
    "    # for calculation of mae and the amount of correctly guessed movies\n",
    "    for index, row in predictions.iterrows():\n",
    "        if row[0] in ogRatings.movieId.unique():\n",
    "            evalCount +=1\n",
    "            mae += abs(row[2] - ogRatings[ogRatings.movieId == row[0]].rating.values[0])\n",
    "        diversity += abs(avSim - row[3])\n",
    "        \n",
    "    # averaging some values    \n",
    "    if evalCount ==0:\n",
    "        mae = np.nan\n",
    "        \n",
    "    else:  \n",
    "        mae /= evalCount \n",
    "    \n",
    "    evalCount /= 20\n",
    "    diversity /= 20*avSim\n",
    "    \n",
    "    # store to the test table\n",
    "    row = pd.DataFrame({'userId':userId,'numPredictions':numPredictions,'accuracy':evalCount, 'meanAverageError':mae, 'explainability':explan,'diversity':diversity}, index = [0])\n",
    "    testTable = pd.concat([testTable,row])\n",
    "    testTable.to_csv('s' + str(size)+'testTable.csv')\n",
    "    \n",
    "'''print('number of correctly predicted movies =' ,evalCount*20)\n",
    "print('accuracy =',evalCount*100, \"%\")\n",
    "print('mean average error of rating predictions = ', mae)\n",
    "print('percentage of explainable recommendations =',explan *100,'%')\n",
    "print('diversity score =',diversity*100, \"%\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad79f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make non personalised version\n",
    "\n",
    "# find a user with a lot of ratings \n",
    "# get rid of some of their ratings \n",
    "# precision = correctly recommended content/ total content\n",
    "# diversity = sum of(similarity - averagesimilarity) / number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf4a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagrams graphs\n",
    "# accuracy of the rating predictions done by multiplying the average rating with (final eval * something that gives it a max of 5)\n",
    "# diversity measured by how many different genres\n",
    "# diversity also can be measured by the similarity metric\n",
    "\n",
    "\n",
    "# 10 users\n",
    "# 2176 - 3221\n",
    "# 4740 - 1224\n",
    "# 3402 - 919\n",
    "# 5440 - 689\n",
    "# 356 - 578\n",
    "# 1721 - 419\n",
    "# 2964 - 248\n",
    "# 1140 - 130 \n",
    "# 3223 - 52\n",
    "# 2792 - 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310a88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582d0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
